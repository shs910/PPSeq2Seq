import torch
import os
import time
import copy
import numpy as np
import torch.nn.functional as F
from tqdm import trange
from operator import add
from torch.autograd import Variable
from classifier import ClassifierHead
from typing import List, Optional, Tuple, Union

SMALL_CONST = 1e-15
BIG_CONST = 1e10

def to_var(x, requires_grad=False, volatile=False, device='cuda'):
    if torch.cuda.is_available() and device == 'cuda':
        x = x.cuda()
    elif device != 'cuda':
        x = x.to(device)
    return Variable(x, requires_grad=requires_grad, volatile=volatile)

def top_k_filter(logits, k, probs=False):
    """
    Masks everything but the k top entries as -infinity (1e10).
    Used to mask logits such that e^-infinity -> 0 won't contribute to the
    sum of the denominator.
    """
    # å°†top-kçš„ä½ç½®å¤–è®¾ç½®ä¸ºinfï¼Œä½¿å¾—exp(-inf)~0,å¯¹åˆ†æ¯æ— è´¡çŒ®ï¼ˆsoftmaxï¼‰
    if k == 0:
        return logits
    else:
        values = torch.topk(logits, k)[0]
        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)
        # å°†å°äºçš„é¢„æµ‹maskæ‰ï¼ˆè®¾ç½®ä¸º0/-inftyï¼‰
        if probs:
            return torch.where(logits < batch_mins,
                               torch.ones_like(logits) * 0.0, logits)
        return torch.where(logits < batch_mins,
                           torch.ones_like(logits) * -BIG_CONST,
                           logits)

def get_classifier(class_size,embed_size,device: str,classifier_model_path):

    classifier = ClassifierHead(
        class_size=class_size,
        embed_size=embed_size
    ).to(device)
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    classifier.load_state_dict(
        torch.load(classifier_model_path, map_location=device))
    classifier.eval()
    print('classifier loaded!!')

    return classifier
    
def tuple2list4past(past,seq_len):
    # shape tuple(tupe(4*tensor))
    '''
    ä¿ç•™decoderçš„KV
    '''
    decoder_past=[]
    encoder_past=[]
    n_layers=len(past)
    # print('n_layers',n_layers)
    for i in range(n_layers):
        decoder_kv=[]
        encoder_kv=[]
        for j in range(4):
            # print(past[i][j].shape[2])
            if past[i][j].shape[2]!=seq_len or (past[i][j].shape[2]==seq_len and j <2):
                decoder_kv.append(past[i][j])
            else:
                encoder_kv.append(past[i][j])
        encoder_past.append(torch.stack(encoder_kv))
        decoder_past.append(torch.stack(decoder_kv))

    return encoder_past,decoder_past
def list2tuple4past(encoder_past,decoder_past):
    ''''''
    n_layers=len(encoder_past)
    # print('n_layers',n_layers)
    past=[]
    for i in range(n_layers):
        kv_item=tuple(x for x in decoder_past[i])+tuple(x for x in encoder_past[i])
        past.append(kv_item)

    past=tuple(past)
    # print('====back===')
    # print(len(past))
    # print(len(past[0]))
    return past

def perturb_past(   past,
                    model,
                    last,
                    t_step,
                    encoder_outputs,
                    unpert_past,
                    unpert_logits,
                    accumulated_hidden,
                    grad_norms,
                    stepsize,
                    classifier,
                    unfinished_sequences,
                    one_hot_bows_vectors,
                    args,
                    token_loss=0,
                ):
    # print('====before===')
    # print(len(past))
    # print(len(past[0]))
    # if t_step<2:
    #     print('='*7,'past_k_v','='*7)
    #     print('len')
    seq_len=encoder_outputs[0].shape[1]
    #é¢„å¤„ç†past_KV ä¸º[n_lyasers*tensor(4,batch_size, num_heads, sequence_length, embed_size_per_head)]
    encoder_past,past=tuple2list4past(past,seq_len=seq_len)
    # åˆå§‹åŒ– \Delta H_t
    
    grad_accumulator = [
        (np.zeros(p.shape).astype("float32"))
        for p in past
    ]
    if accumulated_hidden is None:
        #æœ€åä¸€å±‚hidden_state(è¿­ä»£æ—¶éœ€è¦ç´¯åŠ ä¿®æ”¹)
        accumulated_hidden = 0
    
    if args.decay:
        decay_mask = torch.arange(0.,1.0 + SMALL_CONST,1.0 / (args.window_length))[1:]
    else:
        decay_mask = 1.0
    # print(past[0].shape)
    _, _, _, curr_length, _ = past[0].shape

    if curr_length > args.window_length and args.window_length > 0:
        # ä»…å¯¹çª—å£å†…çš„Hè¿›è¡Œä¼˜åŒ–ï¼ˆä¼šæ»‘åŠ¨ï¼Ÿï¼‰
        ones_key_val_shape = (
                tuple(past[0].shape[:-2])
                + tuple([args.window_length])
                + tuple(past[0].shape[-1:])
        )

        zeros_key_val_shape = (
                tuple(past[0].shape[:-2])
                + tuple([curr_length - args.window_length])
                + tuple(past[0].shape[-1:])
        )

        ones_mask = torch.ones(ones_key_val_shape)
        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)
        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)

        window_mask = torch.cat(
            (ones_mask, torch.zeros(zeros_key_val_shape)),
            dim=-2
        ).to(args.device)
    else:
        # å…¨éƒ¨ä¼˜åŒ–
        window_mask = torch.ones_like(past[0]).to(args.device)
    # accumulate perturbations for num_iterations
    loss_per_iter = []
    new_accumulated_hidden = None
    # è¿­ä»£è®¡ç®— \Delta H_t
    perturb_token_loss=0.0
    count=0
    for i in range(args.num_iterations):
        count+=1
        # print("Iteration ", i + 1)
        curr_perturbation = [
            to_var(torch.from_numpy(p_), requires_grad=True, device=args.device)
            for p_ in grad_accumulator
        ]
        # Compute hidden using perturbed past
        perturbed_past = list(map(add, past, curr_perturbation))

        perturbed_past=list2tuple4past(encoder_past,perturbed_past)

        _, _, _, curr_length, _ = curr_perturbation[0].shape
        # è®¡ç®—ä¿®æ”¹åLMçš„è¾“å‡ºï¼ˆï¼‰(logit,past_KV,hidden_state)past_KVå¿½ç•¥
        # print(last.max())
        # print(la)
        outputs= model(encoder_outputs=encoder_outputs,\
                decoder_input_ids = last, past_key_values=perturbed_past,return_dict=True)
        #print(outputs.keys())
        all_logits, all_hidden =outputs.logits,outputs.decoder_hidden_states
        hidden = all_hidden[-1]
        new_accumulated_hidden = accumulated_hidden + torch.sum(
            hidden,
            dim=1
        ).detach()
        #ä¿®æ”¹åçš„é¢„æµ‹
        logits = all_logits[:, -1, :]  # æœ€åä¸€ä¸ªï¼šå½“å‰é¢„æµ‹çš„token
        probs = F.softmax(logits, dim=-1)

        loss = 0.0
        loss_list = []
    

        # è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒçš„KLæŸå¤±
        kl_loss = 0.0
        if args.kl_scale > 0.0:
            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)
            unpert_probs = (
                    unpert_probs + SMALL_CONST *
                    (unpert_probs <= SMALL_CONST).float().to(args.device).detach()
            )
            # ä¿®æ­£äº†
            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(args.device).detach()
            corrected_probs = probs + correction.detach()
            # print('corrected_probs:' ,corrected_probs.shape)
            # print('corrected_probs[0]:' ,corrected_probs[0].unsqueeze(0).shape)
            # print('unpert_probs:',unpert_probs.shape)
            # print('unpert_probs[0]:',unpert_probs[0].unsqueeze(0).shape)
            # kl_loss = torch.sum(args.kl_scale * (
            #     (corrected_probs * (corrected_probs / unpert_probs).log())
            # ),dim=1)
            # if verbosity_level >= VERY_VERBOSE:
            #     print(' kl_loss', kl_loss.data.cpu().numpy())
            # print('kl_loss: ',kl_loss )
            
            # print('loss: ',loss)
            

        
        # if verbosity_level >= VERBOSE:
        #     print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())

        # compute gradients
        # æ¯ä¸ªsampleçš„lossåˆ†åˆ«è®¡ç®—å¹¶å›ä¼ 
        # bathçš„lossä¸è¿›è¡Œå¹³å‡
        # éœ€è¦è®¡ç®—åˆ†ç±»å™¨æŸå¤±æ—¶
        if not args.bag_of_words or args.loss_type!=2:
            curr_unpert_past = unpert_past
            curr_probs = torch.unsqueeze(probs, dim=1)

            # è·å¾—æ¨¡å‹embeddingå±‚
            if args.model_type=='pegasus':
                wte=model.model.resize_token_embeddings()
            elif args.model_type=='t5':
                wte = model.resize_token_embeddings()
            #  å¤šæ¬¡æ±‚å¹³å‡ï¼šï¼Ÿï¼Ÿï¼Ÿ
            for _ in range(args.horizon_length):
                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)
                
                outputs= model(
                    encoder_outputs=encoder_outputs,
                    past_key_values=curr_unpert_past,
                    decoder_inputs_embeds=inputs_embeds,
                    return_dict=True
                )
                curr_unpert_past, curr_all_hidden=outputs.past_key_values,outputs.decoder_hidden_states
                curr_hidden = curr_all_hidden[-1]
                new_accumulated_hidden = new_accumulated_hidden + torch.sum(
                    curr_hidden, dim=1)
            # t1=time.perf_counter()
            prediction = classifier(new_accumulated_hidden /
                                        (curr_length + 1 + args.horizon_length))
            batch_size=prediction.shape[0]
            label = torch.tensor( batch_size* [args.class_label],
                                    device=args.device,
                                    dtype=torch.long)
            # print('prediction ',prediction.shape)
            # print('prediction[0] ',prediction[0].unsqueeze(0).shape)
            # print('label ',label.shape)
            # print('label[0] ',label[0].unsqueeze(0).shape)
            # print('discrim_loss:',discrim_loss)
            #print(" pplm_discrim_loss:", discrim_loss.data.cpu().numpy())
        num_layers=len(grad_accumulator)
        # =======bath å•ç‹¬è®¡ç®—ä¿®æ”¹ä»£ç éƒ¨åˆ†=========
        # grad_norms=[]
        if not args.loss_not_divded :
            for batch_idx in range(batch_size):
                # å¦‚æœå·²ç»åœæ­¢ç”Ÿæˆï¼Œåˆ™ä¸å†è®¡ç®—è¯¥éƒ¨åˆ†
                if unfinished_sequences[batch_idx] == 0:
                    continue
                # è®¡ç®—æŸå¤±
                indx_window_mask=window_mask[:,batch_idx,:,:,:].unsqueeze(dim=1)
                # è®¡ç®—æŸå¤±
                ce_loss = torch.nn.CrossEntropyLoss()
                discrim_loss = ce_loss(prediction[batch_idx].unsqueeze(0), label[batch_idx].unsqueeze(0))
                kl_loss = args.kl_scale * (
                    (corrected_probs[batch_idx].unsqueeze(0) * \
                        (corrected_probs[batch_idx].unsqueeze(0) / unpert_probs[batch_idx].unsqueeze(0)).log()).sum()
                )
                loss=discrim_loss+kl_loss
                
                if batch_idx==batch_size-1:
                    loss.backward()
                else:
                    loss.backward(retain_graph=True)
                idx_curr_perturbation_grad=[layer_curr_perturbation.grad[:,batch_idx,:,:,:].unsqueeze(dim=1)\
                    for layer_curr_perturbation in curr_perturbation]
                # print('idx_curr_perturbation_grad[0]: ',idx_curr_perturbation_grad[0].shape)
                idx_grad_accumulator=[np.expand_dims(layer_grad_accumulator[:,batch_idx,:,:,:],axis=1) \
                    for layer_grad_accumulator in grad_accumulator]
                # print('idx_grad_accumulator[0]: ',idx_grad_accumulator[0].shape)
                # è®¡ç®—normå’Œæ¢¯åº¦ç­‰ï¼š
                idx_grad_norms = [
                    (torch.norm(grad * indx_window_mask) + SMALL_CONST)
                    for index, grad in enumerate(idx_curr_perturbation_grad)
                    ]
                idx_grad = [
                        -stepsize *
                        (grad * indx_window_mask / idx_grad_norms[
                            index] ** args.gamma).data.cpu().numpy()
                        for index, grad in enumerate(idx_curr_perturbation_grad)
                    ]
                # print('idx_grad_accumulator[0]:',idx_grad_accumulator[0].shape)
                # print('grad:',idx_grad[0].max())
                idx_grad_accumulator = list(map(add, idx_grad, idx_grad_accumulator))
                # print('idx_grad_accumulator[0] after shape',idx_grad_accumulator[0].shape)
                # æ›´æ–°åŸæ¥çš„grad_accumulator
                for layer_idx in range(num_layers):
                    grad_accumulator[layer_idx][:,batch_idx,:,:,:]=np.squeeze(idx_grad_accumulator[layer_idx],axis=1)
                # æ¢¯åº¦åœ¨æœ€åæ¸…é›¶
            # =======bath å•ç‹¬è®¡ç®—ä¿®æ”¹ä»£ç éƒ¨åˆ†=========
        else:
            # batch-meançš„æ–¹æ³•è®¡ç®—loss

        # è®¡ç®—æ¢¯åº¦ï¼Œå¹¶æ›´æ–°\Delta H_t += \alpha * gradient
        # print('len_curr_perturbation: ',len(curr_perturbation))
        # print('curr_perturbation[0]: ',curr_perturbation[0].shape)
        # æºç 
            if args.bag_of_words and args.loss_type!=1:
                # print('BowæŸå¤±')
                for one_hot_bow in one_hot_bows_vectors:
                    # print('probs;size {}'.format(probs.shape))
                    # print('one_hot_bow;size {}'.format(one_hot_bow.shape))
                    # exit()
                    bow_logits = torch.mm(probs, torch.t(one_hot_bow))
                    bow_loss = -torch.log(torch.sum(bow_logits))
                    loss += bow_loss
                    loss_list.append(bow_loss)
            if args.loss_type!=2:
                # è®¡ç®—æŸå¤±
                # print('äº¤å‰ä¸ŠæŸå¤±')
                ce_loss = torch.nn.CrossEntropyLoss()
                discrim_loss = ce_loss(prediction, label)
                loss += discrim_loss
                loss_list.append(discrim_loss)

            kl_loss = args.kl_scale * (
                    (corrected_probs * (corrected_probs / unpert_probs).log()).sum()
                )
            loss += kl_loss
            loss_per_iter.append(loss.mean().data.cpu().numpy())
            loss.backward()
            # print('loss:',loss)
            # è®¡ç®—grad_normsï¼š
            if args.loss_type==2 and grad_norms is not None:
                # print('grad_norms is not None')
                grad_norms = [
                        torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))
                        for index, p_ in enumerate(curr_perturbation)
                    ]
            else:
                # print('grad_norms')
                grad_norms = [
                    (torch.norm(p_.grad * window_mask) + SMALL_CONST)
                    for index, p_ in enumerate(curr_perturbation)
                ]
            # print('grad_norms:',grad_norms[0])
        # normalize gradients
        # \alpha * gradient
        # æºç 
            grad = [
                -stepsize *
                (p_.grad * window_mask / grad_norms[
                    index] ** args.gamma).data.cpu().numpy()
                for index, p_ in enumerate(curr_perturbation)
            ]
            # if args.loss_type==2 :
            #     grad=list(map(lambda x:x*7,grad))
        # print('grad:',grad[0].shape)
        # accumulate gradient
        # \Delta H_t += [\alpha * gradient]
        # æºç 
            grad_accumulator = list(map(add, grad, grad_accumulator))
        # print('len grad_accumulator:',len(grad_accumulator))
        # print('grad_accumulator[0].sise',grad_accumulator[0].shape)
        # exit()
        # reset gradients, just to make sure
        # æ¢¯åº¦æ¸…é›¶
        for p_ in curr_perturbation:
            p_.grad.data.zero_()
        # ////ä¸€æ¬¡æ¢¯åº¦è®¡ç®—+æ¸…0é›¶
        # removing past from the graph(detach,è¿­ä»£é‡æ–°ä½¿ç”¨)
        # å€¼æœªå˜
        new_past = []
        for p_ in past:
            new_past.append(p_.detach())
        past = new_past
        # t2=time.perf_counter()
        # print('loss and back using {}'.format(t2-t1))
        #æƒ³æŸ¥çœ‹loss
        perturb_token_loss+=loss.mean()

    # apply the accumulated perturbations to the past
    grad_accumulator = [
        to_var(torch.from_numpy(p_), requires_grad=True, device=args.device)
        for p_ in grad_accumulator
    ]
    # H_t+=\Delta H_t
    pert_past = list(map(add, past, grad_accumulator))

    pert_past=list2tuple4past(encoder_past,pert_past)
    # print('loss:',perturb_token_loss.item()/count)
    token_loss+=perturb_token_loss.item()/count
    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter,token_loss


def generate_text_pplm(model,tokenizer,classifier,input_text,args,perturb=False,bow_indices=None):
    '''
    input_text={}:åˆ†è¯åçš„æ¨¡å‹è¾“å…¥
    '''
    if not args.not_log:
        print('perturb:',perturb)
    #åˆ†è¯ï¼š
    # input_textä¸ºbatch data
    # max_length=args.max_length
    # collect one hot vectors for bags of words
    one_hot_bows_vectors=None
    if args.loss_type!=1 and args.bag_of_words and perturb:
        one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer,args,
                                                      args.device)
    input_text=tokenizer(input_text,padding=True,truncation=True,max_length=args.max_length,return_tensors='pt').to(args.device)
    batch_size,input_seq_len=input_text.input_ids.shape[0],input_text.input_ids.shape[1]
    output_so_far = None  # è®°å½•æ¨¡å‹çš„è¾“å‡ºid
    decoder_start_token_id=model.config.decoder_start_token_id
    output_so_far=[[decoder_start_token_id] for _ in range(batch_size)]
    output_so_far=torch.tensor(output_so_far,dtype=torch.int64).to(args.device)

    grad_norms = None
    last = None
    unpert_discrim_loss = 0
    loss_in_time = []
    if not args.not_log:
        range_func = trange(args.length, ascii=True)
    else:
        range_func = range(args.length)

    # endocerè·å¾—encoder-hidden_states
    if args.model_type=='pegasus':
        encoder=model.get_encoder()
        encoder_outputs = encoder(
                input_ids=input_text.input_ids,
                attention_mask=input_text.attention_mask,
                return_dict=True,output_hidden_states=True
            )
        # print(encoder_outputs.keys())
    elif args.model_type=='t5':
        encoder_outputs = model.encoder(
                input_ids=input_text.input_ids,
                attention_mask=input_text.attention_mask,
                return_dict=True,output_hidden_states=True
            )
    #     print(type(encoder_outputs))
    #     print(encoder_outputs.keys())
    # ['last_hidden_state', 'hidden_states']
    # ç”¨äºæ£€æµ‹é‚£äº›å¥å­è¿˜æ²¡æœ‰ç”Ÿæˆå®Œæ¯•
    unfinished_sequences = output_so_far.new(output_so_far.shape[0]).fill_(1)
    # past åˆå§‹åŒ–None
    # è¿›è¡Œdecoder
    past=None
    if not args.not_log:
        print('begin generation')
    tokens_loss_mean=[]
    for i in range_func:
        if past is None and output_so_far is not None:
            last = output_so_far[:, -1:]
            # ä¸å­˜åœ¨past_key_valuesæ—¶
            decoder_outputs= model(encoder_outputs=encoder_outputs,\
                decoder_input_ids=last,return_dict=True)
            past=decoder_outputs.past_key_values
            # è¯¥ç‰ˆæœ¬çš„transformersè¿”å›ï¼š
            # logitsã€past_key_valuesã€hidden_states
            #(batch_size, sequence_length, config.vocab_size)
            # [(2, batch_size, num_heads, sequence_length, embed_size_per_head) * n_layers]
            # (batch_size, sequence_length, hidden_size)
        
        # æ™®é€šçš„ç”Ÿæˆ
        outputs = model(encoder_outputs=encoder_outputs,\
                decoder_input_ids=output_so_far,return_dict=True)
        #print(outputs.keys())
        unpert_logits, unpert_past, unpert_all_hidden=outputs.logits,outputs.past_key_values\
            ,outputs.decoder_hidden_states
        # print(type(unpert_all_hidden))
        # print(len(unpert_all_hidden))
        unpert_last_hidden = unpert_all_hidden[-1]
        # if i < 2:
        #     print('last_hidden:size',unpert_last_hidden.shape)


        # check if we are abowe grad max length
        if i >= args.grad_length:
            current_stepsize = args.stepsize * 0
        else:
            current_stepsize = args.stepsize

        # modify the past if necessary
        if not perturb or args.num_iterations == 0:
            # ä¸ä¿®æ”¹å†…éƒ¨çŠ¶æ€ğŸ¤
            pert_past = past

        else:
            accumulated_hidden = unpert_last_hidden[:, :-1, :]
            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)
            # if i<2:
            #     print('accumulated_hidden size ',accumulated_hidden.shape)
            if past is not None and i>=args.strat_perturb:
                # æœ€åˆçš„past is None
                pert_past, _, grad_norms, loss_this_iter,token_loss = perturb_past(   
                    past,
                    model,
                    last,
                    t_step=i,
                    encoder_outputs=encoder_outputs,
                    unpert_past=unpert_past,
                    unpert_logits=unpert_logits,
                    accumulated_hidden=accumulated_hidden,
                    grad_norms=grad_norms,
                    stepsize=current_stepsize,
                    classifier=classifier,
                    unfinished_sequences=unfinished_sequences,
                    one_hot_bows_vectors=one_hot_bows_vectors,
                    args=args,
                    token_loss=0.0
                )
                tokens_loss_mean.append(token_loss)
                loss_in_time.append(loss_this_iter)
                # print('loss_this_iter ',np.mean(loss_this_iter))
            else:
                pert_past = past
        
        # modify pastçš„KVåå†è¾“å…¥é¢„æµ‹
        # last ä¸ºä¹‹å‰çš„tokenç›¸å…³å‚æ•°ï¼Œpastä¸ºä¿®æ”¹åçš„K-V
        out= (decoder_input_ids=last, \
            past_key_values=pert_past,encoder_outputs=encoder_outputs,\
                return_dict=True)
        # print(out.keys())
        # perturbåçš„pastä»£æ›¿ä¹‹å‰çš„past
        pert_logits, past, pert_all_hidden=out.logits,out.past_key_values,out.decoder_hidden_states
        # temperatureï¼šç¼©æ”¾
        pert_logits = pert_logits[:, -1, :] / args.temperature  # + SMALL_CONST
        pert_probs = F.softmax(pert_logits, dim=-1)

        if classifier is not None:
            #è®¡ç®— p(a|x)çš„æŸå¤±
            ce_loss = torch.nn.CrossEntropyLoss()
            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))
            label = torch.tensor([args.class_label for _ in range(batch_size)], device=args.device,
                                 dtype=torch.long)
            unpert_discrim_loss = ce_loss(prediction, label)
            if not args.not_log:
                print("unperturbed discrim loss",unpert_discrim_loss.data.cpu().numpy())
        else:
            unpert_discrim_loss = 0
        
        # Fuse the modified model and original model
        if perturb:
            # ä¿®æ”¹KVåï¼ŒPost-norm Geometric Mean Fusionï¼ï¼ï¼ï¼
            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)

            pert_probs = ((pert_probs ** args.gm_scale) * (
                    unpert_probs ** (1 - args.gm_scale)))  # + SMALL_CONST
            
            pert_probs = top_k_filter(pert_probs, k=args.top_k,
                                      probs=True)  # + SMALL_CONST

            # rescaleï¼ï¼ï¼
            if torch.sum(pert_probs) <= 1:
                pert_probs = pert_probs / torch.sum(pert_probs)

        else:
            pert_logits = top_k_filter(pert_logits, k=args.top_k)  # + SMALL_CONST
            pert_probs = F.softmax(pert_logits, dim=-1)

        # sample or greedy
        if not args.not_sample:
            last = torch.multinomial(pert_probs, num_samples=1)
        else:
            _, last = torch.topk(pert_probs, k=1, dim=-1)
        #å…ˆé“ºå¹³ï¼š
        last=last.reshape(-1)
        #è¿½è¸ªå·²ç»ç”Ÿæˆå®Œæ¯•çš„æ•°æ®
        last = last * unfinished_sequences + args.pad_token_id * (1 - unfinished_sequences)
        # if eos_token was found in one sentence, set sentence to finished
        unfinished_sequences = unfinished_sequences.mul((last != args.eos_token_id).long())
        last=last[:,None]
        
        
        # update context/output_so_far appending the new token
        output_so_far = (
            last if output_so_far is None
            else torch.cat((output_so_far, last), dim=1)
        )
        if not args.not_log:
            print(tokenizer.decode(output_so_far.tolist()[0]))
        # åˆ¤æ–­æ˜¯å¦æ‰€æœ‰å¥å­å‡ç”Ÿæˆå®Œæ¯•ï¼Œé€€å‡ºç”Ÿæˆï¼š
        if unfinished_sequences.max() == 0:
            break
    
    return output_so_far, unpert_discrim_loss, loss_in_time,np.mean(tokens_loss_mean)

def get_paraphrases(sentence,tokenizer,model, prefix="paraphrase: ", n_predictions=5, top_k=120, max_length=256,device="cuda"):
        text = prefix + sentence + " </s>"
        encoding = tokenizer.encode_plus(
            text, pad_to_max_length=True, return_tensors="pt"
        )
        input_ids, attention_masks = encoding["input_ids"].to(device), encoding[
            "attention_mask"
        ].to(device)

        model_output = model.generate(
            input_ids=input_ids,
            attention_mask=attention_masks,
            do_sample=True,
            max_length=max_length,
            top_k=top_k,
            top_p=0.98,
            early_stopping=True,
            num_return_sequences=n_predictions,
        )

        outputs = []
        for output in model_output:
            generated_sent = tokenizer.decode(
                output, skip_special_tokens=True, clean_up_tokenization_spaces=True
            )
            if (
                generated_sent.lower() != sentence.lower()
                and generated_sent not in outputs
            ):
                outputs.append(generated_sent)
        return outputs

def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> \
        List[List[List[int]]]:
    bow_indices = []
    for id_or_path in bag_of_words_ids_or_paths:
        filepath = id_or_path
        with open(filepath, "r") as f:
            words = f.read().strip().split("\n")
        bow_indices.append(
            [tokenizer.encode(word.strip(),
                              add_special_tokens=False)
             for word in words])
    # print('===========bow_indices=============')
    # print('[n_file*[n_line*[n_line_words]]]')
    # print(len(bow_indices))
    # for id in bow_indices[0]:
    #     print(tokenizer.decode(id))
    # print('='*20)
    return bow_indices

def build_bows_one_hot_vectors(bow_indices, tokenizer, args,device='cuda'):
    if bow_indices is None:
        return None
    # print('='*7,'one_hot_bows_vectors','='*7)
    one_hot_bows_vectors = []
    for single_bow in bow_indices:
        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))
        # åªä¿ç•™äº†å•ä¸ªwordsçš„è¯æ±‡
        single_bow = torch.tensor(single_bow).to(device)
        num_words = single_bow.shape[0]
        print(num_words,tokenizer.vocab_size)
        one_hot_bow = torch.zeros(num_words, args.vocab_size).to(device)
        one_hot_bow.scatter_(1, single_bow, 1)
        one_hot_bows_vectors.append(one_hot_bow)

    return one_hot_bows_vectors

def full_text_generation(model,tokenizer,classifier,input_text,bag_of_words,args):
    # ç”Ÿæˆä¸ºåŸå§‹æ–‡æœ¬
    unpert_gen_tok_text=['nothing']
    # BoW
    bow_indices = []
    if bag_of_words and args.loss_type!=1:
        print(bag_of_words.split(";"))
        bow_indices = get_bag_of_words_indices(bag_of_words.split(";"),tokenizer)
    unpert_loss=0
    if not args.not_gen_both:
        unpert_gen_tok_text, _, _ ,unpert_loss= generate_text_pplm(
            model=model,
            tokenizer=tokenizer,
            input_text=input_text,
            classifier=classifier,
            args=args,
            perturb=False
        )
    if args.device == 'cuda':
        torch.cuda.empty_cache()
    
    # ç”Ÿæˆä¿®æ”¹åçš„æ–‡æœ¬
    pert_gen_tok_texts = []
    discrim_losses = []
    losses_in_time = []
    for i in range(args.num_samples):
        # past==None
        pert_gen_tok_text, discrim_loss, loss_in_time ,pert_loss= generate_text_pplm(
            model=model,
            tokenizer=tokenizer,
            input_text=input_text,
            classifier=classifier,
            args=args,
            bow_indices=bow_indices,
            perturb=True,
        )
        pert_gen_tok_texts.append(pert_gen_tok_text)
        if classifier is not None:
            discrim_losses.append(discrim_loss.data.cpu().numpy())
        losses_in_time.append(loss_in_time)

    if args.device == 'cuda':
        torch.cuda.empty_cache()

    return unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time,unpert_loss,pert_loss

def load_data(src_path,bath_size,model_type):
    print('laoding data....')
    with open(src_path,'r',encoding='utf-8') as f:
        data=[]
        bath_data=[]
        bath_count=0
        total_leng=0
        for idx,line in enumerate(f.readlines()):
            line=line.strip('\n')
            total_leng=idx+1
            if model_type=='t5':
                bath_data.append("paraphrase: "+line+" </s>")
            elif model_type=='pegasus':
                bath_data.append(line)
            bath_count+=1
            if bath_count==bath_size:
                data.append(copy.deepcopy(bath_data))
                bath_count=0
                # print(bath_data)
                bath_data.clear()
        if bath_data:
            data.append(bath_data)
        print('total_num:{},bath_szie:{},num_bath:{}'.format(\
            total_leng,bath_size,len(data)))
        # print(data)
        # exit()
    return data

def save_results(unpert_data,pert_data,tgt_dir,title):
    unpert_path=os.path.join(tgt_dir,title+'withoutClassifier.txt')
    pert_path=os.path.join(tgt_dir,title+'withClassifier.txt')
    if len(unpert_data)!=len(pert_data):
        unpert_data=[['nothing'] for _ in range(len(pert_data))]
    with open(unpert_path,'a',encoding='utf-8') as unpert,\
        open(pert_path,'a',encoding='utf-8') as pert:
        #print(unpert_data,pert_data)
        for un_text,texts in zip(unpert_data,pert_data):
            for text in un_text:
                unpert.write(text+'\n')
            # batch infer æ—¶ï¼Œunpert_dataä¹Ÿæ˜¯bathï¼ï¼ï¼
            for text in texts:
                pert.write(text+'\n')